- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)  
- [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)  
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)  
- [A TIME SERIES IS WORTH 64 WORDS: LONG-TERM FORECASTING WITH TRANSFORMERS](https://arxiv.org/abs/2211.14730)  
- [Are Transformers Effective for Time Series Forecasting?](https://arxiv.org/abs/2205.13504)  
- [N-BEATS: Neural Basis Expansion Analysis for Interpretable Time Series Forecasting](https://arxiv.org/abs/1905.10437)  
- [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2501.12948)  
- [Attention Is All You Need (PDF)](https://arxiv.org/pdf/1706.03762)  
- [LLaMA 3.1 Research Paper](https://arxiv.org/abs/2407.21783)  
- [QWEN2 Technical Report](https://arxiv.org/pdf/2407.10671v4)  
- [DeepSeek-Coder](https://arxiv.org/pdf/2401.14196v2)  
- [Phi-3 Technical Report](https://arxiv.org/pdf/2404.14219v4)  
- [Mixtral of Experts](https://arxiv.org/pdf/2401.04088v1)  
- [Large Language Models: A Survey](https://arxiv.org/pdf/2402.06196v2)  
- [KAN: Kolmogorovâ€“Arnold Networks](https://arxiv.org/pdf/2404.19756v4)  
