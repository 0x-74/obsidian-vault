https://arxiv.org/pdf/1901.10323
https://arxiv.org/pdf/1412.0767
https://arxiv.org/pdf/1611.05431v2

https://arxiv.org/abs/2308.12966

https://arxiv.org/pdf/1910.01108

https://arxiv.org/pdf/2211.14730
https://arxiv.org/pdf/2205.13504
https://arxiv.org/pdf/1905.10437


#### Advanced Topics & Papers
- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762)  
- [LLaMA 3.1 Research Paper](https://arxiv.org/abs/2407.21783)  
- **Recent ArXiv Papers:**  
  - [QWEN2 Technical Report](https://arxiv.org/pdf/2407.10671v4)  
  - [DeepSeek-Coder](https://arxiv.org/pdf/2401.14196v2)  
  - [Phi-3 Technical Report](https://arxiv.org/pdf/2404.14219v4)  
  - [Mixtral of Experts](https://arxiv.org/pdf/2401.04088v1)  
  - [Large Language Models: A Survey](https://arxiv.org/pdf/2402.06196v2)  
  - [KAN: Kolmogorovâ€“Arnold Networks](https://arxiv.org/pdf/2404.19756v4)  